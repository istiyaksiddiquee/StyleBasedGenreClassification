{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import time \n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(a, b):\n",
    "    \n",
    "    row_list =[]     \n",
    "\n",
    "    for index, rows in a.iterrows(): \n",
    "        \n",
    "        my_list = [rows[i] for i in a.columns]     \n",
    "        row_list.append(my_list) \n",
    "    \n",
    "    target = [rows for _, rows in b.items()]\n",
    "        \n",
    "    return row_list, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                          book_id  avg_sentence_length  avg_char_per_word  \\\n0                    pg19459.epub               0.0445             4.5419   \n1         pg12CarolGlassLook.epub               0.0735             4.0510   \n2                     pg9311.epub               0.0601             4.1475   \n3                    pg34849.epub               0.0375             4.4684   \n4                    pg17698.epub               0.0799             4.2311   \n..                            ...                  ...                ...   \n89                    pg3828.epub               0.0812             4.2460   \n90                   pg15895.epub               0.0601             4.1501   \n91                    pg5662.epub               0.0792             3.9478   \n92  pg19337DickensChristCarl.epub               0.0694             4.2876   \n93                   pg33945.epub               0.0706             4.2827   \n\n    avg_punctuation_per_sentence  long_word_ratio  noun_ratio  verb_ratio  \\\n0                         2.8576           0.2117      0.2211      0.1761   \n1                         4.2273           0.1166      0.2020      0.2110   \n2                         3.9024           0.1327      0.2232      0.1958   \n3                         4.5543           0.1912      0.2367      0.1809   \n4                         2.8501           0.1571      0.2164      0.1939   \n..                           ...              ...         ...         ...   \n89                        2.7203           0.1770      0.2253      0.1916   \n90                        3.2832           0.1323      0.2209      0.1893   \n91                        2.9638           0.0979      0.2053      0.2056   \n92                        3.5462           0.1669      0.2390      0.1855   \n93                        3.0414           0.1553      0.2347      0.1855   \n\n    adverb_ratio  preposition_ratio  adjective_ratio  article_ratio  \\\n0         0.0599             0.1384           0.0854         0.0784   \n1         0.0785             0.1146           0.0565         0.0764   \n2         0.0696             0.1156           0.0722         0.0621   \n3         0.0529             0.1315           0.0721         0.0754   \n4         0.0683             0.1246           0.0612         0.0738   \n..           ...                ...              ...            ...   \n89        0.0482             0.1209           0.0676         0.0737   \n90        0.0553             0.1191           0.0694         0.0808   \n91        0.0640             0.0937           0.0675         0.0663   \n92        0.0544             0.1252           0.0718         0.0764   \n93        0.0587             0.1209           0.0780         0.0685   \n\n    gerund_infinitive_ratio  downtoner_ratio  amplifier_ratio  \\\n0                    0.0211           0.0031           0.0033   \n1                    0.0249           0.0046           0.0050   \n2                    0.0146           0.0032           0.0031   \n3                    0.0194           0.0027           0.0030   \n4                    0.0178           0.0047           0.0036   \n..                      ...              ...              ...   \n89                   0.0177           0.0021           0.0016   \n90                   0.0163           0.0025           0.0014   \n91                   0.0136           0.0028           0.0041   \n92                   0.0187           0.0015           0.0032   \n93                   0.0152           0.0018           0.0014   \n\n    demonstrative_ratio  type_token_ratio  genre  \n0                0.0191            0.0070      5  \n1                0.0147            0.0127      5  \n2                0.0164            0.0060      5  \n3                0.0199            0.0114      5  \n4                0.0175            0.0012      5  \n..                  ...               ...    ...  \n89               0.0117            0.0039      4  \n90               0.0123            0.0155      1  \n91               0.0118            0.0824      2  \n92               0.0164            0.0193      2  \n93               0.0144            0.0067      1  \n\n[94 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>avg_sentence_length</th>\n      <th>avg_char_per_word</th>\n      <th>avg_punctuation_per_sentence</th>\n      <th>long_word_ratio</th>\n      <th>noun_ratio</th>\n      <th>verb_ratio</th>\n      <th>adverb_ratio</th>\n      <th>preposition_ratio</th>\n      <th>adjective_ratio</th>\n      <th>article_ratio</th>\n      <th>gerund_infinitive_ratio</th>\n      <th>downtoner_ratio</th>\n      <th>amplifier_ratio</th>\n      <th>demonstrative_ratio</th>\n      <th>type_token_ratio</th>\n      <th>genre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pg19459.epub</td>\n      <td>0.0445</td>\n      <td>4.5419</td>\n      <td>2.8576</td>\n      <td>0.2117</td>\n      <td>0.2211</td>\n      <td>0.1761</td>\n      <td>0.0599</td>\n      <td>0.1384</td>\n      <td>0.0854</td>\n      <td>0.0784</td>\n      <td>0.0211</td>\n      <td>0.0031</td>\n      <td>0.0033</td>\n      <td>0.0191</td>\n      <td>0.0070</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pg12CarolGlassLook.epub</td>\n      <td>0.0735</td>\n      <td>4.0510</td>\n      <td>4.2273</td>\n      <td>0.1166</td>\n      <td>0.2020</td>\n      <td>0.2110</td>\n      <td>0.0785</td>\n      <td>0.1146</td>\n      <td>0.0565</td>\n      <td>0.0764</td>\n      <td>0.0249</td>\n      <td>0.0046</td>\n      <td>0.0050</td>\n      <td>0.0147</td>\n      <td>0.0127</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>pg9311.epub</td>\n      <td>0.0601</td>\n      <td>4.1475</td>\n      <td>3.9024</td>\n      <td>0.1327</td>\n      <td>0.2232</td>\n      <td>0.1958</td>\n      <td>0.0696</td>\n      <td>0.1156</td>\n      <td>0.0722</td>\n      <td>0.0621</td>\n      <td>0.0146</td>\n      <td>0.0032</td>\n      <td>0.0031</td>\n      <td>0.0164</td>\n      <td>0.0060</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pg34849.epub</td>\n      <td>0.0375</td>\n      <td>4.4684</td>\n      <td>4.5543</td>\n      <td>0.1912</td>\n      <td>0.2367</td>\n      <td>0.1809</td>\n      <td>0.0529</td>\n      <td>0.1315</td>\n      <td>0.0721</td>\n      <td>0.0754</td>\n      <td>0.0194</td>\n      <td>0.0027</td>\n      <td>0.0030</td>\n      <td>0.0199</td>\n      <td>0.0114</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pg17698.epub</td>\n      <td>0.0799</td>\n      <td>4.2311</td>\n      <td>2.8501</td>\n      <td>0.1571</td>\n      <td>0.2164</td>\n      <td>0.1939</td>\n      <td>0.0683</td>\n      <td>0.1246</td>\n      <td>0.0612</td>\n      <td>0.0738</td>\n      <td>0.0178</td>\n      <td>0.0047</td>\n      <td>0.0036</td>\n      <td>0.0175</td>\n      <td>0.0012</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>pg3828.epub</td>\n      <td>0.0812</td>\n      <td>4.2460</td>\n      <td>2.7203</td>\n      <td>0.1770</td>\n      <td>0.2253</td>\n      <td>0.1916</td>\n      <td>0.0482</td>\n      <td>0.1209</td>\n      <td>0.0676</td>\n      <td>0.0737</td>\n      <td>0.0177</td>\n      <td>0.0021</td>\n      <td>0.0016</td>\n      <td>0.0117</td>\n      <td>0.0039</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>pg15895.epub</td>\n      <td>0.0601</td>\n      <td>4.1501</td>\n      <td>3.2832</td>\n      <td>0.1323</td>\n      <td>0.2209</td>\n      <td>0.1893</td>\n      <td>0.0553</td>\n      <td>0.1191</td>\n      <td>0.0694</td>\n      <td>0.0808</td>\n      <td>0.0163</td>\n      <td>0.0025</td>\n      <td>0.0014</td>\n      <td>0.0123</td>\n      <td>0.0155</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>pg5662.epub</td>\n      <td>0.0792</td>\n      <td>3.9478</td>\n      <td>2.9638</td>\n      <td>0.0979</td>\n      <td>0.2053</td>\n      <td>0.2056</td>\n      <td>0.0640</td>\n      <td>0.0937</td>\n      <td>0.0675</td>\n      <td>0.0663</td>\n      <td>0.0136</td>\n      <td>0.0028</td>\n      <td>0.0041</td>\n      <td>0.0118</td>\n      <td>0.0824</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>pg19337DickensChristCarl.epub</td>\n      <td>0.0694</td>\n      <td>4.2876</td>\n      <td>3.5462</td>\n      <td>0.1669</td>\n      <td>0.2390</td>\n      <td>0.1855</td>\n      <td>0.0544</td>\n      <td>0.1252</td>\n      <td>0.0718</td>\n      <td>0.0764</td>\n      <td>0.0187</td>\n      <td>0.0015</td>\n      <td>0.0032</td>\n      <td>0.0164</td>\n      <td>0.0193</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>pg33945.epub</td>\n      <td>0.0706</td>\n      <td>4.2827</td>\n      <td>3.0414</td>\n      <td>0.1553</td>\n      <td>0.2347</td>\n      <td>0.1855</td>\n      <td>0.0587</td>\n      <td>0.1209</td>\n      <td>0.0780</td>\n      <td>0.0685</td>\n      <td>0.0152</td>\n      <td>0.0018</td>\n      <td>0.0014</td>\n      <td>0.0144</td>\n      <td>0.0067</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>94 rows × 17 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "address = \"E:\\\\DKE related\\\\Studies\\\\Second Semester\\\\ATiML\\\\Codes\\\\Semester Project\\\\feature_vector.csv\"\n",
    "\n",
    "df = pd.read_csv(address)\n",
    "df['genre'] = df['genre'].astype('category')\n",
    "df['genre'] = df['genre'].cat.codes\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(df.iloc[:, 1:-1].to_numpy(), df.iloc[:, -1].to_numpy(),random_state=0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[5.2800e-02, 4.1324e+00, 3.0140e+00, ..., 2.6000e-03, 2.3900e-02,\n        9.4000e-03],\n       [6.3000e-02, 4.3200e+00, 3.3884e+00, ..., 1.3000e-03, 1.6000e-02,\n        6.1000e-03],\n       [8.2100e-02, 4.1984e+00, 3.2589e+00, ..., 5.0000e-03, 1.4400e-02,\n        5.8000e-03],\n       ...,\n       [6.5800e-02, 4.1549e+00, 3.4849e+00, ..., 3.2000e-03, 2.2400e-02,\n        2.4000e-03],\n       [8.5000e-02, 3.9235e+00, 3.0112e+00, ..., 8.0000e-04, 2.0300e-02,\n        1.7100e-02],\n       [3.7600e-02, 4.3843e+00, 4.6118e+00, ..., 4.1000e-03, 2.3900e-02,\n        2.4000e-03]])"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_data = train_data.shuffle(buffer_size=500).batch(50).repeat(200)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_data = test_data.batch(64)\n",
    "\n",
    "# for item in train_data:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 400 steps\nEpoch 1/3\n400/400 [==============================] - 1s 3ms/step - loss: 8.5917\nEpoch 2/3\n400/400 [==============================] - 1s 2ms/step - loss: 7.7417\nEpoch 3/3\n400/400 [==============================] - 1s 2ms/step - loss: 7.5492\n1/1 [==============================] - 0s 65ms/step - loss: 5.4625\n['loss'] 5.4625420570373535\n"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(50, activation=\"relu\"),\n",
    "    keras.layers.Dense(50, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "model.fit(train_data, epochs=3)\n",
    "\n",
    "result = model.evaluate(test_data)\n",
    "print(model.metrics_names, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20)\n",
    "# mse_test = model.evaluate(X_test, y_test)\n",
    "# X_new = X_test[:3]\n",
    "# y_pred = model.predict(X_new)\n",
    "\n",
    "\n",
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"E:\\\\DKE related\\\\Studies\\\\Second Semester\\\\ATiML\\\\Codes\\\\Semester Project\\\\feature_vector.csv\"\n",
    "\n",
    "df = pd.read_csv(address)\n",
    "df['genre'] = df['genre'].astype('category')\n",
    "df['genre'] = df['genre'].cat.codes\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:-1].to_numpy()\n",
    "Y = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X.astype(np.float64))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(df.iloc[:, 1:-1], df.iloc[:, -1], test_size=0.20, random_state = 0, shuffle=True, stratify = df.iloc[:, -1])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train_index, val_index in skf.split(X_train_val, Y_train_val):\n",
    "    \n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "\n",
    "    print(X_train.shape, X_val.shape, Y_train.shape, Y_val.shape)\n",
    "\n",
    "    nb = MultinomialNB()\n",
    "\n",
    "    %time nb.fit(X_train, Y_train)\n",
    "\n",
    "    y_pred_nb = nb.predict(X_val)\n",
    "    acc_nb = metrics.accuracy_score(y_val, y_pred_nb)\n",
    "    print(acc_nb)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((70, 15), (24, 15), (70,), (24,))"
     },
     "metadata": {},
     "execution_count": 164
    }
   ],
   "source": [
    "X = df.iloc[:, 1:-1].to_numpy()\n",
    "Y = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state = 0, shuffle=True, stratify = Y)\n",
    "\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 964 µs\n0.375\n"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "%time nb.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_nb = nb.predict(X_test)\n",
    "\n",
    "acc_nb = metrics.accuracy_score(Y_test, Y_pred_nb)\n",
    "print(acc_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "fitting data to SVM\nWall time: 21 ms\nfitting data to Logistic\nWall time: 57.8 ms\nfitting data to Naive Bayes\nWall time: 1.01 ms\nfitting data to SVM\nWall time: 14 ms\nfitting data to Logistic\nWall time: 21 ms\nfitting data to Naive Bayes\nWall time: 1.02 ms\nfitting data to SVM\nWall time: 15 ms\nfitting data to Logistic\nWall time: 20 ms\nfitting data to Naive Bayes\nWall time: 0 ns\nfitting data to SVM\nWall time: 13 ms\nfitting data to Logistic\nWall time: 19 ms\nfitting data to Naive Bayes\nWall time: 1.01 ms\nfitting data to SVM\nWall time: 14 ms\nfitting data to Logistic\nWall time: 19 ms\nfitting data to Naive Bayes\nWall time: 999 µs\nAverage Validation Accuracy\nSVM: 0.2267; Logistic: 0.24; NB: 0.2; RB: 0.3067\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "val_acc_svm = 0\n",
    "val_acc_logistic = 0\n",
    "val_acc_nb = 0\n",
    "val_acc_rf = 0\n",
    "\n",
    "X = df.iloc[:, 1:-1].to_numpy()\n",
    "Y = df.iloc[:, -1].to_numpy()\n",
    "\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.20, random_state = 0, shuffle=True, stratify = Y)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train_index, val_index in skf.split(X_train_val, Y_train_val):\n",
    "    \n",
    "    X_train, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "    Y_train, Y_val = Y_train_val[train_index], Y_train_val[val_index]\n",
    "    \n",
    "    val_acc_svm += perform_SVM(X_train, Y_train, X_val, Y_val)\n",
    "    val_acc_logistic += perform_Logistic(X_train, Y_train, X_val, Y_val)\n",
    "    val_acc_nb += perform_NB(X_train, Y_train, X_val, Y_val)\n",
    "    val_acc_rf += perform_random_forrest(X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "    # print(\"SVM: {}; Logistic: {}; NB: {}; RB: {}\".format(round(acc_svm, 4), round(acc_logistic, 4), round(acc_nb, 4), round()))\n",
    "\n",
    "val_acc_svm = float(val_acc_svm/5)\n",
    "val_acc_logistic = float(val_acc_logistic/5)\n",
    "val_acc_nb = float(val_acc_nb/5) \n",
    "val_acc_rf = float(val_acc_rf/5) \n",
    "\n",
    "print(\"Average Validation Accuracy\")\n",
    "print(\"SVM: {}; Logistic: {}; NB: {}; RB: {}\".format(round(val_acc_svm, 4), round(val_acc_logistic, 4), round(val_acc_nb, 4), round(val_acc_rf, 4)))\n",
    "\n",
    "# test accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_SVM(X_train, Y_train, X_val, Y_val):\n",
    "    # SVM \n",
    "    \n",
    "    svm_clf = Pipeline([        \n",
    "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "    ])\n",
    "\n",
    "    print(\"fitting data to SVM\")\n",
    "    # svm_clf = SVC(gamma='auto')\n",
    "    %time svm_clf.fit(X_train, Y_train)\n",
    "    Y_pred_svm = svm_clf.predict(X_val)\n",
    "    \n",
    "    return metrics.accuracy_score(Y_val, Y_pred_svm)\n",
    "\n",
    "def perform_Logistic(X_train, Y_train, X_val, Y_val):\n",
    "    \n",
    "    softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\n",
    "    print(\"fitting data to Logistic\")\n",
    "    %time softmax_reg.fit(X_train, Y_train)\n",
    "    Y_pred_logistic = softmax_reg.predict(X_val)    \n",
    "    return metrics.accuracy_score(Y_val, Y_pred_logistic)\n",
    "\n",
    "    # Multinomial Naive Bayes\n",
    "\n",
    "def perform_NB(X_train, Y_train, X_val, Y_val):\n",
    "\n",
    "    nb = MultinomialNB()\n",
    "    print(\"fitting data to Naive Bayes\")\n",
    "    %time nb.fit(X_train, Y_train)\n",
    "    Y_pred_nb = nb.predict(X_val)\n",
    "    return metrics.accuracy_score(Y_val, Y_pred_nb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "def perform_random_forrest(X_train, Y_train, X_val, Y_val):\n",
    "\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    Y_pred = clf.predict(X_val)\n",
    "    return metrics.accuracy_score(Y_val, Y_pred)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python",
   "display_name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}